{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japan challenge submission template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantopian.pipeline import Pipeline, CustomFactor\n",
    "from quantopian.pipeline.data import EquityPricing, factset\n",
    "from quantopian.pipeline.factors import Returns, SimpleMovingAverage\n",
    "from quantopian.pipeline.domain import (\n",
    "    AT_EQUITIES, # Austria\n",
    "    AU_EQUITIES, # Australia\n",
    "    BE_EQUITIES, # Belgium\n",
    "    BR_EQUITIES, # Brazil\n",
    "    CA_EQUITIES, # Canada\n",
    "    CH_EQUITIES, # Switzerland\n",
    "    CN_EQUITIES, # China\n",
    "    DE_EQUITIES, # Germany\n",
    "    DK_EQUITIES, # Denmark\n",
    "    ES_EQUITIES, # Spain\n",
    "    FI_EQUITIES, # Finland\n",
    "    FR_EQUITIES, # France\n",
    "    GB_EQUITIES, # Great Britain\n",
    "    HK_EQUITIES, # Hong Kong\n",
    "    IE_EQUITIES, # Ireland\n",
    "    IN_EQUITIES, # India\n",
    "    IT_EQUITIES, # Italy\n",
    "    JP_EQUITIES, # Japan\n",
    "    KR_EQUITIES, # South Korea\n",
    "    NL_EQUITIES, # Netherlands\n",
    "    NO_EQUITIES, # Norway\n",
    "    NZ_EQUITIES, # New Zealand\n",
    "    PT_EQUITIES, # Portugal\n",
    "    SE_EQUITIES, # Sweden\n",
    "    SG_EQUITIES, # Singapore\n",
    "    US_EQUITIES, # United States\n",
    ")\n",
    "from quantopian.research import run_pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import empyrical as ep\n",
    "import alphalens as al\n",
    "import pyfolio as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class filter_extend(CustomFactor):\n",
    "    window_length = 15\n",
    "    def compute(self, today, asset_ids, out, values):\n",
    "        out[:] = np.max(values, axis=0)\n",
    "\n",
    "def evaluate_factor(factor, \n",
    "                    domain, \n",
    "                    start_date, \n",
    "                    end_date,\n",
    "                    factor_screen=None,\n",
    "                    quantiles=5,\n",
    "                    returns_lengths=(1, 5, 10)):\n",
    "    \"\"\"Analyze a Pipeline Factor using Alphalens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    factor : quantopian.pipeline.factors.Factor\n",
    "        Factor producing scores to be evaluated.\n",
    "    domain : quantopian.pipeline.domain.Domain\n",
    "        Domain on which the factor should be evaluated.\n",
    "    start_date : str or pd.Timestamp\n",
    "        Start date for evaluation period.\n",
    "    end_date : str or pd.Timestamp\n",
    "        End date for evaluation period.\n",
    "    standardize : \n",
    "    factor_screen : quantopian.pipeline.filters.Filter, optional\n",
    "        Filter defining which assets ``factor`` should be evaluated on.\n",
    "        Default is ``factor.notnull()``.\n",
    "    quantiles : int, optional\n",
    "        Number of buckets to use for quantile groups. Default is 5\n",
    "    returns_lengths : sequence[int]\n",
    "        Forward-returns horizons to use when evaluating ``factor``. \n",
    "        Default is 1-day, 5-day, and 10-day returns.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    factor_data : pd.DataFrame\n",
    "        A (date, asset)-indexed DataFrame with the following columns:\n",
    "            'factor': float64\n",
    "                Values produced by ``factor``.\n",
    "            'factor_quantiles': int64\n",
    "                Daily quantile label for each\n",
    "    \"\"\"\n",
    "    calendar = domain.calendar\n",
    "    # Roll input dates to the next trading session.\n",
    "    start_date = calendar.minute_to_session_label(pd.Timestamp(start_date, tz='UTC'))\n",
    "    end_date = calendar.minute_to_session_label(pd.Timestamp(end_date, tz='UTC'))\n",
    "    \n",
    "    if factor_screen is None:\n",
    "        factor_screen = factor.notnull()\n",
    "        \n",
    "    # Run pipeline to get factor values and quantiles.\n",
    "    factor_pipe = Pipeline(\n",
    "        {'factor': factor, \n",
    "         'factor_quantile': factor.quantiles(quantiles, mask=factor_screen)},\n",
    "        screen=factor_screen,\n",
    "        domain=domain,\n",
    "    )\n",
    "    factor_results = run_pipeline(factor_pipe, start_date, end_date, chunksize=250)\n",
    "    \n",
    "    column_order = []\n",
    "    returns_cols = {}\n",
    "    for length in returns_lengths:\n",
    "        colname = '{}D'.format(length)\n",
    "        column_order.append(colname)\n",
    "        # Here we are not computing cumulative returns, this could be done\n",
    "        # more efficiently\n",
    "        returns_cols[colname] = Returns(window_length=2)\n",
    "    \n",
    "#     returns_pipe = Pipeline(returns_cols, domain=domain)\n",
    "    returns_pipe = Pipeline(returns_cols, domain=domain, screen = filter_extend(inputs = [univ_filter]).eq(1))\n",
    "\n",
    "    # Compute returns for the period after the factor pipeline, then \n",
    "    # shift the results back to align with our factor values.\n",
    "    returns_start_date = start_date\n",
    "    returns_end_date = end_date + domain.calendar.day * max(returns_lengths)\n",
    "    raw_returns = run_pipeline(returns_pipe, returns_start_date, returns_end_date, chunksize=500)\n",
    "    \n",
    "    shifted_returns = {}\n",
    "    for name, length in zip(column_order, returns_lengths):\n",
    "        # Shift 1-day returns back by a day, 5-day returns back by 5 days, etc.\n",
    "        raw = raw_returns[name]\n",
    "        shifted_returns[name] = backshift_returns_series(raw, length)\n",
    "        \n",
    "    # Merge backshifted returns into a single frame indexed like our desired output.\n",
    "    merged_returns = pd.DataFrame(\n",
    "        data=shifted_returns, \n",
    "        index=factor_results.index, \n",
    "        columns=column_order,\n",
    "    )\n",
    "    \n",
    "    # Concat factor results and forward returns column-wise.\n",
    "    merged = pd.concat([factor_results, merged_returns], axis=1)\n",
    "    merged.index.set_names(['date', 'asset'], inplace=True)\n",
    "    \n",
    "    # Drop NaNs\n",
    "    merged = merged.dropna(how='any')\n",
    "    \n",
    "    # Add a Business Day Offset to the DateTimeIndex\n",
    "    merged.index.levels[0].freq = pd.tseries.offsets.BDay()\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def backshift_returns_series(series, N):\n",
    "    \"\"\"Shift a multi-indexed series backwards by N observations in the first level.\n",
    "    \n",
    "    This can be used to convert backward-looking returns into a forward-returns series.\n",
    "    \"\"\"\n",
    "    ix = series.index\n",
    "    dates, sids = ix.levels\n",
    "    date_labels, sid_labels = map(np.array, ix.labels)\n",
    "    # Output date labels will contain the all but the last N dates.\n",
    "    new_dates = dates[:-N]\n",
    "    # Output data will remove the first M rows, where M is the index of the\n",
    "    # last record with one of the first N dates.\n",
    "    cutoff = date_labels.searchsorted(N)\n",
    "    new_date_labels = date_labels[cutoff:] - N\n",
    "    new_sid_labels = sid_labels[cutoff:]\n",
    "    new_values = series.values[cutoff:]\n",
    "    assert new_date_labels[0] == 0\n",
    "    new_index = pd.MultiIndex(\n",
    "        levels=[new_dates, sids],\n",
    "        labels=[new_date_labels, new_sid_labels],\n",
    "        sortorder=1,\n",
    "        names=ix.names,\n",
    "    )\n",
    "    return pd.Series(data=new_values, index=new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import empyrical as ep\n",
    "import alphalens as al\n",
    "import pyfolio as pf\n",
    "\n",
    "def compute_turnover(df):\n",
    "    return df.dropna().unstack().dropna(how='all').fillna(0).diff().abs().sum(1)\n",
    "\n",
    "def get_max_median_position_concentration(expos):\n",
    "    longs = expos.loc[expos > 0]\n",
    "    shorts = expos.loc[expos < 0]\n",
    "\n",
    "    return expos.groupby(level=0).quantile([.05, .25, .5, .75, .95]).unstack()\n",
    "\n",
    "def compute_factor_stats(factor_data_total, periods=range(1, 15)):\n",
    "    portfolio_returns_total = al.performance.factor_returns(factor_data_total)\n",
    "    portfolio_returns_total.columns = portfolio_returns_total.columns.map(lambda x: int(x[:-1]))\n",
    "    for i in portfolio_returns_total.columns:\n",
    "        portfolio_returns_total[i] = portfolio_returns_total[i].shift(i)\n",
    "\n",
    "    delay_sharpes_total = portfolio_returns_total.apply(ep.sharpe_ratio)\n",
    "    \n",
    "    factor = factor_data_total.factor\n",
    "    turnover = compute_turnover(factor)\n",
    "    n_holdings = factor.groupby(level=0).count()\n",
    "    perc_holdings = get_max_median_position_concentration(factor)\n",
    "    \n",
    "    return {'factor_data_total': factor_data_total, \n",
    "            'portfolio_returns_total': portfolio_returns_total,\n",
    "            'delay_sharpes_total': delay_sharpes_total,\n",
    "            'turnover': turnover,\n",
    "            'n_holdings': n_holdings,\n",
    "            'perc_holdings': perc_holdings,\n",
    "    }\n",
    "\n",
    "def plot_overview_tear_sheet(factor_data, periods=range(1, 15)):\n",
    "    # We assume portfolio weights, so make sure factor scores sum to 1\n",
    "    factor_data['factor'] = factor_data.factor.div(factor_data.abs().groupby(level='date').sum()['factor'])\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    gs = plt.GridSpec(3, 4)\n",
    "    ax1 = plt.subplot(gs[0:2, 0:2])\n",
    "    \n",
    "    factor_stats = compute_factor_stats(factor_data, periods=periods)\n",
    "                         \n",
    "    pd.DataFrame({'total': factor_stats['delay_sharpes_total']}).plot.bar(ax=ax1)\n",
    "    ax1.set(xlabel='delay', ylabel='IR')\n",
    "\n",
    "    ax2a = plt.subplot(gs[0:2, 2:4])\n",
    "    delay_cum_rets_total = factor_stats['portfolio_returns_total'][list(range(1, 5))].apply(ep.cum_returns)\n",
    "    delay_cum_rets_total.plot(ax=ax2a)\n",
    "    ax2a.set(title='Total returns', ylabel='Cumulative returns')\n",
    "    \n",
    "    ax6 = plt.subplot(gs[-1, 0:2])\n",
    "    factor_stats['n_holdings'].plot(color='b', ax=ax6)\n",
    "    ax6.set_ylabel('# holdings', color='b')\n",
    "    ax6.tick_params(axis='y', labelcolor='b')\n",
    "    \n",
    "    ax62 = ax6.twinx()\n",
    "    factor_stats['turnover'].plot(color='r', ax=ax62)\n",
    "    ax62.set_ylabel('turnover', color='r')\n",
    "    ax62.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    ax7 = plt.subplot(gs[-1, 2:4])\n",
    "    factor_stats['perc_holdings'].plot(ax=ax7)\n",
    "    ax7.set(ylabel='Long/short perc holdings')\n",
    "    \n",
    "    gs.tight_layout(fig)\n",
    "    \n",
    "    return fig, factor_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universe definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom factor that gets the minimum volume traded over the last two weeks.\n",
    "class MinVolume(CustomFactor):\n",
    "    inputs=[EquityPricing.volume]\n",
    "    window_length=10\n",
    "    def compute(self, today, asset_ids, out, values):\n",
    "        # Calculates the column-wise standard deviation, ignoring NaNs\n",
    "        out[:] = np.min(values, axis=0)\n",
    "\n",
    "# Create a volume and price filter that filters for stocks in the top 30%.\n",
    "# We multiply by price to rule out penny stocks that trade in huge volume.\n",
    "volume_min = MinVolume()\n",
    "price = EquityPricing.close.latest\n",
    "univ_filter = ((price * volume_min).percentile_between(70, 100, mask=(volume_min > 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your alpha factor here. Make sure to delete the following cell before making your submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "WIN_LIMIT = 0\n",
    "\n",
    "def signalize(df):\n",
    "    z = (df.rank() - 0.5)/df.count()\n",
    "    return z.replace(np.nan, z.mean())\n",
    "\n",
    "def preprocess(a):\n",
    "    a = a.astype(np.float64)\n",
    "    a[np.isinf(a)] = np.nan\n",
    "    a = np.nan_to_num(a - np.nanmean(a))\n",
    "    a = winsorize(a, limits=[WIN_LIMIT,WIN_LIMIT])\n",
    "    b = preprocessing.scale(a)\n",
    "    \n",
    "    return b/sum(abs(b))\n",
    "    \n",
    "\n",
    "class Volatility(CustomFactor):  \n",
    "    inputs = [EquityPricing.close]  \n",
    "    window_length = 22  \n",
    "    def compute(self, today, assets, out, close):  \n",
    "        # [0:-1] is needed to remove last close since diff is one element shorter  \n",
    "        daily_returns = np.diff(close, axis = 0) / close[0:-1]  \n",
    "        out[:] = daily_returns.std(axis = 0) * math.sqrt(252)\n",
    "        \n",
    "class MarketCap(CustomFactor):\n",
    "    # Pre-declare inputs and window_length\n",
    "    inputs = [EquityPricing.close, factset.Fundamentals.shs_float_cf]\n",
    "    window_length = 1\n",
    "    \n",
    "    # Compute market cap value\n",
    "    def compute(self, today, assets, out, close, shares):\n",
    "        out[:] = close[-1] * shares[-1]\n",
    "\n",
    "class mean_rev(CustomFactor):   \n",
    "    inputs = [EquityPricing.high,EquityPricing.low,EquityPricing.close]\n",
    "    window_length = 30\n",
    "    window_safe = True\n",
    "    def compute(self, today, assets, out, high, low, close):\n",
    "\n",
    "        p = (high+low+close)/3\n",
    "\n",
    "        m = len(close[0,:])\n",
    "        n = len(close[:,0])\n",
    "        b = np.zeros(m)\n",
    "        a = np.zeros(m)\n",
    "\n",
    "        for k in range(10,n+1):\n",
    "            price_rel = np.nanmean(p[-k:,:],axis=0)/p[-1,:]\n",
    "            wt = np.nansum(price_rel)\n",
    "            b += wt*price_rel\n",
    "            price_rel = 1.0/price_rel\n",
    "            wt = np.nansum(price_rel)\n",
    "            a += wt*price_rel\n",
    "\n",
    "        out[:] = preprocess(b-a)\n",
    "        \n",
    "class LastEightQuartersStability(CustomFactor):\n",
    "    # Get the last 4 reported values of the given asof_date + field.\n",
    "#     outputs = ['q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8']\n",
    "    window_length = 66*8\n",
    "\n",
    "    def compute(self, today, assets, out, asof_date, values):\n",
    "        for column_ix in range(asof_date.shape[1]):\n",
    "            _, unique_indices = np.unique(asof_date[:, column_ix], return_index=True)\n",
    "            quarterly_values = values[unique_indices, column_ix]\n",
    "            if len(quarterly_values) < 8:\n",
    "                quarterly_values = np.hstack([\n",
    "                    np.repeat([np.nan], 8 - len(quarterly_values)),\n",
    "                    quarterly_values,\n",
    "                ])\n",
    "            quarterly_values = quarterly_values[-8:]\n",
    "            out[column_ix] = np.mean(quarterly_values)/np.std(quarterly_values)\n",
    "        out[:] = signalize(out[:])\n",
    "            \n",
    "class LastEightQuartersStd(CustomFactor):\n",
    "    # Get the last 4 reported values of the given asof_date + field.\n",
    "#     outputs = ['q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8']\n",
    "    window_length = 66*8\n",
    "\n",
    "    def compute(self, today, assets, out, asof_date, values):\n",
    "        for column_ix in range(asof_date.shape[1]):\n",
    "            _, unique_indices = np.unique(asof_date[:, column_ix], return_index=True)\n",
    "            quarterly_values = values[unique_indices, column_ix]\n",
    "            if len(quarterly_values) < 8:\n",
    "                quarterly_values = np.hstack([\n",
    "                    np.repeat([np.nan], 8 - len(quarterly_values)),\n",
    "                    quarterly_values,\n",
    "                ])\n",
    "            quarterly_values = quarterly_values[-8:]\n",
    "            out[column_ix] = np.std(quarterly_values)\n",
    "            \n",
    "class LastEightQuartersMean(CustomFactor):\n",
    "    # Get the last 4 reported values of the given asof_date + field.\n",
    "#     outputs = ['q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8']\n",
    "    window_length = 66*8\n",
    "\n",
    "    def compute(self, today, assets, out, asof_date, values):\n",
    "        for column_ix in range(asof_date.shape[1]):\n",
    "            _, unique_indices = np.unique(asof_date[:, column_ix], return_index=True)\n",
    "            quarterly_values = values[unique_indices, column_ix]\n",
    "            if len(quarterly_values) < 8:\n",
    "                quarterly_values = np.hstack([\n",
    "                    np.repeat([np.nan], 8 - len(quarterly_values)),\n",
    "                    quarterly_values,\n",
    "                ])\n",
    "            quarterly_values = quarterly_values[-8:]\n",
    "            out[column_ix] = np.mean(quarterly_values)\n",
    "         \n",
    "        \n",
    "        \n",
    "class LastEightQuartersPTP(CustomFactor):\n",
    "    # Get the last 4 reported values of the given asof_date + field.\n",
    "#     outputs = ['q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8']\n",
    "    window_length = 66*8\n",
    "\n",
    "    def compute(self, today, assets, out, asof_date, values):\n",
    "        for column_ix in range(asof_date.shape[1]):\n",
    "            _, unique_indices = np.unique(asof_date[:, column_ix], return_index=True)\n",
    "            quarterly_values = values[unique_indices, column_ix]\n",
    "            if len(quarterly_values) < 8:\n",
    "                quarterly_values = np.hstack([\n",
    "                    np.repeat([np.nan], 8 - len(quarterly_values)),\n",
    "                    quarterly_values,\n",
    "                ])\n",
    "            quarterly_values = quarterly_values[-8:]\n",
    "            out[column_ix] = -np.ptp(quarterly_values)\n",
    "         \n",
    "        \n",
    "        \n",
    "sales_growth_stability = LastEightQuartersStability(\n",
    "                                inputs = [factset.Fundamentals.sales_gr_qf_asof_date, \n",
    "                                          factset.Fundamentals.sales_gr_qf], \n",
    "                                mask = univ_filter).rank().demean().\n",
    "\n",
    "\n",
    "gross_margin_stability = LastEightQuartersStability(\n",
    "                                inputs = [factset.Fundamentals.gross_mgn_qf_asof_date, \n",
    "                                          factset.Fundamentals.gross_mgn_qf], \n",
    "                                mask = univ_filter).zscore()\n",
    "\n",
    "\n",
    "capex_vol = LastEightQuartersPTP(\n",
    "                                inputs = [factset.Fundamentals.capex_assets_qf_asof_date, \n",
    "                                          factset.Fundamentals.capex_assets_qf], \n",
    "                                mask = univ_filter).zscore()\n",
    "\n",
    "alpha = sales_growth_stability \n",
    "# + mean_rev(mask = univ_filter) + gross_margin_stability\n",
    "# alpha = capex_vol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor = alpha\n",
    "# domain = JP_EQUITIES\n",
    "# start_date = '2015-06-1' \n",
    "# end_date = '2018-10-1'\n",
    "# factor_screen=univ_filter\n",
    "# quantiles=5\n",
    "\n",
    "# calendar = domain.calendar\n",
    "#     # Roll input dates to the next trading session.\n",
    "# start_date = calendar.minute_to_session_label(pd.Timestamp(start_date, tz='UTC'))\n",
    "# end_date = calendar.minute_to_session_label(pd.Timestamp(end_date, tz='UTC'))\n",
    "    \n",
    "# if factor_screen is None:\n",
    "#     factor_screen = factor.notnull()\n",
    "  \n",
    "# # Run pipeline to get factor values and quantiles.\n",
    "# factor_pipe = Pipeline(\n",
    "#         {'factor': factor, \n",
    "#          'factor_quantile': factor.quantiles(quantiles, mask=factor_screen)},\n",
    "#         screen=factor_screen,\n",
    "#         domain=domain,\n",
    "#     )\n",
    "# factor_results = run_pipeline(factor_pipe, start_date, end_date)\n",
    "    \n",
    "# print factor_results.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call evaluate_factor on our factor to get Alphalens-formatted data.\n",
    "al_data = evaluate_factor(\n",
    "    alpha, \n",
    "    JP_EQUITIES, \n",
    "    '2015-06-1', \n",
    "    '2018-10-1', \n",
    "    factor_screen=univ_filter,\n",
    "    quantiles=5,\n",
    "    returns_lengths=range(1, 15),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overview_tear_sheet(al_data);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
